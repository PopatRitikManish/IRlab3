{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gensim\n",
    "from elasticsearch import Elasticsearch\n",
    "from gensim.models import FastText\n",
    "from sklearn.conftest import fetch_20newsgroups\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_docs = []\n",
    "for doc in newsgroups.data:\n",
    "    # Tokenize the document\n",
    "    tokens = gensim.utils.simple_preprocess(doc.lower())\n",
    "    # Remove stop words and stem the tokens\n",
    "    stemmed_tokens = [gensim.parsing.porter.PorterStemmer().stem(token) for token in tokens if token not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "    # Join the stemmed tokens back into a string\n",
    "    preprocessed_doc = ' '.join(stemmed_tokens)\n",
    "    preprocessed_docs.append(preprocessed_doc)\n",
    "\n",
    "# Train the FastText model\n",
    "model = FastText(preprocessed_docs, vector_size=300, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Save the model to a binary file\n",
    "model.save('model.bin')\n",
    "\n",
    "# Initialize Elasticsearch client with URL\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "\n",
    "# Delete the index if it already exists\n",
    "index_name = 'my_index'\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "\n",
    "# Create index with appropriate mappings\n",
    "index_mappings = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'text': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'vector': {\n",
    "                'type': 'dense_vector',\n",
    "                'dims': 300\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "es.indices.create(index=index_name, body=index_mappings)\n",
    "\n",
    "# Iterate over preprocessed documents and generate vectors\n",
    "for i, doc in enumerate(preprocessed_docs):\n",
    "    # Split the preprocessed document into tokens\n",
    "    tokens = doc.split()\n",
    "    # Generate the vector for the document by averaging the vectors of its tokens\n",
    "    vector_sum = 0\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector_sum += model.wv[token]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        vector = vector_sum / count\n",
    "        # Store the document and its vector in the Elasticsearch index\n",
    "        es.index(index=index_name, id=i, body={'text': doc, 'vector': vector.tolist()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import gensim\n",
    "import elasticsearch\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "# Define NDCG calculation function\n",
    "def calculate_ndcg(ranked_relevance, k):\n",
    "    # Ideal ranking (perfect relevance)\n",
    "    ideal_ranking = sorted(ranked_relevance, reverse=True)\n",
    "\n",
    "    # Calculate DCG (Discounted Cumulative Gain)\n",
    "    dcg = sum((2**rel - 1) / np.log2(rank + 2) for rank, rel in enumerate(ranked_relevance[:k]))\n",
    "\n",
    "    # Calculate ideal DCG (iDCG)\n",
    "    idcg = sum((2**rel - 1) / np.log2(rank + 2) for rank, rel in enumerate(ideal_ranking[:k]))\n",
    "\n",
    "    # Calculate NDCG\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    return ndcg\n",
    "\n",
    "# Open the CSV file for writing\n",
    "with open('precision_recall_ndcg_map.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Query', 'Precision', 'Recall', 'NDCG', 'MAP', 'Score'])\n",
    "\n",
    "    # Define your Elasticsearch connection (es) and Word2Vec model (model) here\n",
    "\n",
    "    # Load the newsgroups dataset for demonstration\n",
    "    newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "    # Loop over user queries\n",
    "    for user_query in ['nature', 'news', 'sports', 'weather', 'economices', 'stockmarket', 'geography', 'chemistry', 'football', 'basketball', 'book', 'play', 'report', 'knife', 'moon', 'murder', 'evening', 'state', 'county', 'bounty', 'pen', 'mail']:\n",
    "        # Preprocess the user query\n",
    "        tokens = gensim.utils.simple_preprocess(user_query.lower())\n",
    "        stemmed_tokens = [gensim.parsing.porter.PorterStemmer().stem(token) for token in tokens if token not in gensim.parsing.preprocessing.STOPWORDS]\n",
    "        preprocessed_query = ' '.join(stemmed_tokens)\n",
    "\n",
    "        # Search for similar documents using Elasticsearch\n",
    "        search_body = {\n",
    "            'query': {\n",
    "                'script_score': {\n",
    "                    'query': {\n",
    "                        'match_all': {}\n",
    "                    },\n",
    "                    'script': {\n",
    "                        'source': 'cosineSimilarity(params.query_vector, \"vector\") + 1.0',\n",
    "                        'params': {\n",
    "                            'query_vector': model.wv[preprocessed_query.split()].mean(axis=0).tolist()\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            '_source': {\n",
    "                'includes': ['text']\n",
    "            }\n",
    "        }\n",
    "        search_results = es.search(index='my_index', body=search_body)['hits']['hits']\n",
    "        scores = [hit['_score'] for hit in search_results]  # Assuming '_score' field is present in Elasticsearch results\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        relevant_docs = set([i for i, doc in enumerate(newsgroups.data) if user_query in doc.lower()])\n",
    "        retrieved_docs = set([int(hit['_id']) for hit in search_results])\n",
    "        relevant_and_retrieved = relevant_docs.intersection(retrieved_docs)\n",
    "        precision = len(relevant_and_retrieved) / len(retrieved_docs) if len(retrieved_docs) > 0 else 0\n",
    "        recall = len(relevant_and_retrieved) / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "\n",
    "        # Calculate NDCG (Normalized Discounted Cumulative Gain)\n",
    "        ranked_relevance = [1 if int(hit['_id']) in relevant_docs else 0 for hit in search_results]\n",
    "        k = min(len(ranked_relevance), 10)  # Consider the top 10 results for NDCG\n",
    "        ndcg = calculate_ndcg(ranked_relevance, k)\n",
    "\n",
    "        # Calculate and add MAP (Mean Average Precision) score\n",
    "        map_scores = []\n",
    "        cumulative_precision = 0\n",
    "        relevant_count = len(relevant_docs)\n",
    "        for i, hit in enumerate(search_results):\n",
    "            if int(hit['_id']) in relevant_docs:\n",
    "                cumulative_precision += 1\n",
    "                map_scores.append(cumulative_precision / (i + 1))\n",
    "\n",
    "        if map_scores:\n",
    "            map_score = sum(map_scores) / relevant_count\n",
    "        else:\n",
    "            map_score = 0\n",
    "\n",
    "        # Write the Precision, Recall, NDCG, and MAP scores to the CSV file\n",
    "        writer.writerow([user_query, precision, recall, ndcg, map_score])\n",
    "\n",
    "        scores = [hit['_score'] for hit in search_results]  # Assuming '_score' field is present in Elasticsearch results\n",
    "\n",
    "        # Write the precision, recall, and score to the CSV file\n",
    "        for score in scores:\n",
    "            writer.writerow([user_query, precision, recall, ndcg, map_score,score])\n",
    "\n",
    "        # Print the top 10 most similar documents\n",
    "        print(f'Top 10 most similar documents for query \"{user_query}\":')\n",
    "        for i, hit in enumerate(search_results[:10]):\n",
    "            print(f'{i+1}. {hit[\"_source\"][\"text\"]}')\n",
    "\n",
    "        # Print Precision, Recall, NDCG, and MAP\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'NDCG: {ndcg}')\n",
    "        print(f'MAP:{map_score}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
